{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AX36RF2NM8fZ",
    "outputId": "c5a5fdca-e0d6-451c-ddde-b94bf414e553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "81PIbD14eom4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XueyFU6VM0-i",
    "outputId": "48f8608a-a2d7-407a-9c46-3042eac711db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Food total: 10\n",
      "\n",
      "Food category: ['ayam_goreng', 'bakso', 'bubur', 'gado_gado', 'mie_ayam', 'nasi_goreng', 'nasi_padang', 'rawon', 'sate_ayam', 'soto_ayam']\n"
     ]
    }
   ],
   "source": [
    "SOURCE = './'\n",
    "\n",
    "# Main dataset\n",
    "DATASET_PATH = os.path.join(SOURCE, \"raw_dataset\")\n",
    "\n",
    "# Split dataset\n",
    "SPLIT_DATASET_PATH = os.path.join(SOURCE, \"split_dataset\") # Change it\n",
    "\n",
    "TRAIN_PATH = os.path.join(SPLIT_DATASET_PATH, 'train')\n",
    "VAL_PATH = os.path.join(SPLIT_DATASET_PATH, 'val')\n",
    "TEST_PATH = os.path.join(SPLIT_DATASET_PATH, 'test')\n",
    "\n",
    "# Food Categories\n",
    "FOOD_LIST = os.listdir(DATASET_PATH)\n",
    "NUM_OF_FOOD = len(FOOD_LIST)\n",
    "\n",
    "print('Food total:', NUM_OF_FOOD)\n",
    "\n",
    "print('\\nFood category:', FOOD_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veYxGn9MOVk3",
    "outputId": "6af87d43-df2e-4b6b-dd4b-d245e279777e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images\n",
      " 1. ayam_goreng      :   151 \n",
      " 2. bakso            :   148 \n",
      " 3. bubur            :   152 \n",
      " 4. gado_gado        :   148 \n",
      " 5. mie_ayam         :   151 \n",
      " 6. nasi_goreng      :   149 \n",
      " 7. nasi_padang      :   145 \n",
      " 8. rawon            :   148 \n",
      " 9. sate_ayam        :   147 \n",
      "10. soto_ayam        :   150 \n",
      "\n",
      "Total images: 1489\n",
      "Lowest: 145\n",
      "Highest: 152\n",
      "\n",
      "Mean: 148.90\n"
     ]
    }
   ],
   "source": [
    "def print_dataset_info(dataset_path):\n",
    "  IMAGE_COUNT = []\n",
    "  food_list = os.listdir(dataset_path)\n",
    "\n",
    "  print('Number of Images')\n",
    "  for index, food in enumerate(sorted(food_list)):\n",
    "      path = os.path.join(dataset_path, food)\n",
    "      num_images = len(os.listdir(path))\n",
    "      IMAGE_COUNT.append(num_images)\n",
    "\n",
    "      print('{:2}. {:16} : {:5} '.format(index + 1, food, num_images))\n",
    "\n",
    "  IMAGE_COUNT = np.array(IMAGE_COUNT)\n",
    "\n",
    "  print('\\nTotal images: {}'.format(np.sum(IMAGE_COUNT)))\n",
    "  print('Lowest: {}'.format(np.min(IMAGE_COUNT)))\n",
    "  print('Highest: {}'.format(np.max(IMAGE_COUNT)))\n",
    "\n",
    "  print('\\nMean: {:.2f}'.format(np.mean(IMAGE_COUNT)))\n",
    "\n",
    "print_dataset_info(DATASET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GKFb0Ej7TB53"
   },
   "outputs": [],
   "source": [
    "def reset_split_dataset(clean_data_path, train_path, val_path, test_path):\n",
    "    # Delete existing\n",
    "    if os.path.exists(clean_data_path):\n",
    "        shutil.rmtree(clean_data_path)\n",
    "\n",
    "    # Create new empty directory\n",
    "    for food in FOOD_LIST:\n",
    "        train_spice_path = os.path.join(train_path, food)\n",
    "        os.makedirs(train_spice_path)\n",
    "\n",
    "        val_spice_path = os.path.join(val_path, food)\n",
    "        os.makedirs(val_spice_path)\n",
    "\n",
    "        test_spice_path = os.path.join(test_path, food)\n",
    "        os.makedirs(test_spice_path)\n",
    "\n",
    "    print('Reset complete.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "OQ3Z8nCQg6vv"
   },
   "outputs": [],
   "source": [
    "def resize_rename_save(source_dir, temp, food_name, thresh, image_size=(384, 384)):\n",
    "  # Ensure the destination directory exists\n",
    "  os.makedirs(temp, exist_ok=True)\n",
    "\n",
    "  count = 0\n",
    "  for food in os.listdir(source_dir):\n",
    "    # if count >= thresh:\n",
    "    #   break\n",
    "\n",
    "  # Construct the full image path\n",
    "    image_path = os.path.join(source_dir, food)\n",
    "\n",
    "    try:\n",
    "      # Open, resize, and convert the image\n",
    "      with Image.open(image_path) as img:\n",
    "        img = img.convert('RGB').resize(image_size)\n",
    "\n",
    "      # Rename and save\n",
    "      jpeg_name = food_name + str(count).zfill(4) + \".jpeg\"\n",
    "      image_dest_path = os.path.join(temp, jpeg_name)\n",
    "      img.save(image_dest_path, \"JPEG\")\n",
    "\n",
    "      count += 1\n",
    "    except Exception as e:\n",
    "      print(f\"Skipping file {food} due to an error: {e}\")\n",
    "\n",
    "  print(f\"Processed {count} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0KiuES0DTaet"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset_path, train_path, val_path, test_path, split=(0.8, 0.1, 0.1)):\n",
    "    train_size, val_size, test_size = split\n",
    "    if not round(train_size + val_size + test_size, 2) == 1.0:\n",
    "        raise ValueError(\"Split values must add up to 1.0\")\n",
    "    \n",
    "    # Get images\n",
    "    dir_list = os.listdir(dataset_path)\n",
    "    # Remove zero-size images\n",
    "    final_list = [filename for filename in dir_list if os.path.getsize(os.path.join(dataset_path, filename)) > 0]\n",
    "    total_images = len(final_list)\n",
    "    \n",
    "    # Calculate exact split sizes\n",
    "    train_split = int(train_size * total_images)\n",
    "    val_split = int(val_size * total_images)\n",
    "    \n",
    "    # Shuffle the data for random distribution\n",
    "    import random\n",
    "    random.shuffle(final_list)\n",
    "    \n",
    "    # Fixed indexing: Use proper slicing to ensure all images are included\n",
    "    train_files = final_list[:train_split]\n",
    "    val_files = final_list[train_split:(train_split + val_split)]\n",
    "    test_files = final_list[(train_split + val_split):]  # No need to calculate test_split\n",
    "    \n",
    "    # Move files to respective directories\n",
    "    for filename in train_files:\n",
    "        shutil.move(os.path.join(dataset_path, filename), os.path.join(train_path, filename))\n",
    "    for filename in val_files:\n",
    "        shutil.move(os.path.join(dataset_path, filename), os.path.join(val_path, filename))\n",
    "    for filename in test_files:\n",
    "        shutil.move(os.path.join(dataset_path, filename), os.path.join(test_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "6sTcfTckTde3"
   },
   "outputs": [],
   "source": [
    "def create_split_dataset(dataset_path, train_path, val_path, test_path, split, tresh):\n",
    "    temp_folder_path = './tmp'\n",
    "\n",
    "    # Create the temp folder and clean up any existing folder\n",
    "    if os.path.exists(temp_folder_path):\n",
    "        shutil.rmtree(temp_folder_path)\n",
    "    os.makedirs(temp_folder_path, exist_ok=True)\n",
    "\n",
    "    for food in FOOD_LIST:\n",
    "        source_path = os.path.join(dataset_path, food)\n",
    "\n",
    "        train = os.path.join(train_path, food)\n",
    "        val = os.path.join(val_path, food)\n",
    "        test = os.path.join(test_path, food)\n",
    "\n",
    "        # Process and save images to the temp directory\n",
    "        resize_rename_save(source_path, temp_folder_path, food, tresh)\n",
    "\n",
    "        # Split data into training, validation, and test sets\n",
    "        split_dataset(temp_folder_path, train, val, test, split)\n",
    "\n",
    "        # Clean up the temporary folder\n",
    "        shutil.rmtree(temp_folder_path)\n",
    "        print(f'Data processed and splitting completed for: {food} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L5eASAj6TgtR",
    "outputId": "390c7ac1-e6e9-4fa7-b4d7-923171c3d2ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset complete.\n",
      "\n",
      "Processed 151 images.\n",
      "Data processed and splitting completed for: ayam_goreng \n",
      "\n",
      "Processed 148 images.\n",
      "Data processed and splitting completed for: bakso \n",
      "\n",
      "Processed 152 images.\n",
      "Data processed and splitting completed for: bubur \n",
      "\n",
      "Processed 148 images.\n",
      "Data processed and splitting completed for: gado_gado \n",
      "\n",
      "Processed 151 images.\n",
      "Data processed and splitting completed for: mie_ayam \n",
      "\n",
      "Processed 149 images.\n",
      "Data processed and splitting completed for: nasi_goreng \n",
      "\n",
      "Processed 145 images.\n",
      "Data processed and splitting completed for: nasi_padang \n",
      "\n",
      "Processed 148 images.\n",
      "Data processed and splitting completed for: rawon \n",
      "\n",
      "Processed 147 images.\n",
      "Data processed and splitting completed for: sate_ayam \n",
      "\n",
      "Processed 150 images.\n",
      "Data processed and splitting completed for: soto_ayam \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# IMAGE_SIZE = (384, 384)\n",
    "SPLIT_SIZE = (.8, .15, .05)\n",
    "THRESHOLD = None\n",
    "\n",
    "reset_split_dataset(SPLIT_DATASET_PATH, TRAIN_PATH, VAL_PATH, TEST_PATH)\n",
    "create_split_dataset(DATASET_PATH, TRAIN_PATH, VAL_PATH, TEST_PATH, SPLIT_SIZE, THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOlpwbmxTjE7",
    "outputId": "14e40805-374a-4cbc-9f8c-a9e878eea4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in Train: 1178\n",
      "Total Images in Val: 220\n",
      "Total Images in Test: 80\n",
      "\n",
      "Total Images in Clean Dataset: 1478\n",
      "\n",
      "Train Ratio: 0.8\n",
      "Dev Ratio: 0.15\n",
      "Test Ratio: 0.05\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "SPLIT_DATASET_PATH = os.path.join(SOURCE, \"split_dataset_4\") # I've renamed it\n",
    "\n",
    "TRAIN_PATH = os.path.join(SPLIT_DATASET_PATH, 'train')\n",
    "VAL_PATH = os.path.join(SPLIT_DATASET_PATH, 'val')\n",
    "TEST_PATH = os.path.join(SPLIT_DATASET_PATH, 'test')\n",
    "\n",
    "def check_total_images(folder_name, data_path):\n",
    "    total_sum = 0\n",
    "    for rootdir, dirs, files in os.walk(data_path):\n",
    "        for subdir in dirs:\n",
    "            path = os.path.join(rootdir, subdir)\n",
    "            total_sum += len(os.listdir(path))\n",
    "    print('Total Images in {}: {}'.format(folder_name, total_sum))\n",
    "    return total_sum\n",
    "\n",
    "train_count = check_total_images('Train', TRAIN_PATH)\n",
    "dev_count = check_total_images('Val', VAL_PATH)\n",
    "test_count = check_total_images('Test', TEST_PATH)\n",
    "\n",
    "total_count = train_count + dev_count + test_count\n",
    "\n",
    "ratio_train = round(train_count / total_count, 2)\n",
    "ratio_dev = round(dev_count / total_count, 2)\n",
    "ratio_test = round(test_count / total_count, 2)\n",
    "\n",
    "print('\\nTotal Images in Clean Dataset: {}\\n\\nTrain Ratio: {}\\nDev Ratio: {}\\nTest Ratio: {}'.format(total_count, ratio_train, ratio_dev, ratio_test))\n",
    "print(round((ratio_train + ratio_dev + ratio_test), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S_NSWfSESX2j",
    "outputId": "cee57660-6736-47a8-880d-2014afe838a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-Write Successfull\n"
     ]
    }
   ],
   "source": [
    "# Run this for creating a list of categories in .txt file\n",
    "def write_final_categories(txt_file, directory):\n",
    "  categories = os.listdir(directory)\n",
    "\n",
    "  with open(txt_file, 'w') as f:\n",
    "    for item in sorted(categories):\n",
    "      f.write(\"%s\\n\" % item)\n",
    "\n",
    "  print('Re-Write Successfull')\n",
    "\n",
    "write_final_categories('./categories.txt', DATASET_PATH)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
